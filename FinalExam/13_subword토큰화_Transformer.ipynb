{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2fa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 막 만든 예시 문장 - low가 5번, lower가 2번, newest가 6번, widest가 3번 등장하도록 작성\n",
    "train_sentence = ['low lower',\n",
    "                 'low lower newest widest',\n",
    "                 'low newest newest widest',\n",
    "                 'low widest newest',\n",
    "                 'low newest',\n",
    "                 'newest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6987ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"low\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b414fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'o', 'w']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea4e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 단어 사전(dictionary): {'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}\n"
     ]
    }
   ],
   "source": [
    "# 각 단어의 빈도수를 계산하여 사전을 생성\n",
    "count_dict = {} # 단어 빈도수 사전\n",
    "\n",
    "for sentence in train_sentence: # train_sentence의 각 문장이 차례로 들어감\n",
    "    words = sentence.split() # 문장을 단어 리스트로 나눔 ['low','lower']\n",
    "    for word in words: # 문장의 각 단어를 넣음\n",
    "        split_w = \" \".join(list(word)) # 'l o w' 띄워쓰기 join을 함\n",
    "        if split_w in count_dict: # 만약 이렇게 만든 단어가 단어 빈도수 사전에 있다면 개수 추가\n",
    "            count_dict[split_w] = count_dict[split_w] + 1\n",
    "        else:# 없다면 개수 1로 \n",
    "            count_dict[split_w] = 1\n",
    "\n",
    "print(f'초기 단어 사전(dictionary): {count_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dd86e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 단어 집합(vocabulary): ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd']\n"
     ]
    }
   ],
   "source": [
    "# 초기 단어집합 생성\n",
    "vocabulary = []\n",
    "\n",
    "for sentence in train_sentence: # corpus의 문장을 투입\n",
    "    for word in sentence: # 문장의 각 단어로 쪼갬\n",
    "        if word != ' ' and word not in vocabulary: # 만약 공백이 아니고 단어집합에 없다면\n",
    "            vocabulary.append(word) # 단어집합에 추가함\n",
    "\n",
    "print(f'초기 단어 집합(vocabulary): {vocabulary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a936df",
   "metadata": {},
   "source": [
    "get_stats(count_dict) 함수 작성\n",
    ": 각 글자 쌍의 빈도수 계산\n",
    "\n",
    "함수 입력 값\n",
    "* count_dict\n",
    "    * 현재의 단어 빈도 사전(Dictionary)\n",
    "    * 예: {'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}\n",
    "\n",
    "함수 출력 값\n",
    "* pairs\n",
    "    * 단어 빈도 사전 내의 pair들과 빈도수(frequency)\n",
    "    * 예: {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 8, ('e', 'r'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adf7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(count_dict): # 단어 빈도수 사전을 받음\n",
    "    # 각 글자 쌍의 빈도수 계산 예: 'l o' -> 5회, 'o w' -> 5회 등\n",
    "    pairs = {}\n",
    "    for word, freq in count_dict.items(): # 단어 빈도수 사전의 key value를 받음 초기에는 Ex> \"l o w\" , 5 \n",
    "        symbols = word.split() # ['l' , 'o' , 'w'] 공백을 기준으로 split\n",
    "        for i in range(len(symbols)-1):\n",
    "            pair = (symbols[i],symbols[i+1]) # ('l','o')\n",
    "            if pair in pairs: # 위의 pair가 pairs 사전에 있다면 freq를 추가\n",
    "                pairs[pair]+=freq\n",
    "            else: # 위의 pair가 pairs 사전에 없다면 freq로 설정\n",
    "                pairs[pair] = freq\n",
    "    # {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 8, ('e', 'r'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5262f2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(('a','b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ba6505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140d050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(pair, v_in): # 가장 빈번한 글자 쌍과 단어 카운트 사전을 받는다\n",
    "    v_out = {} # 가장 빈번한 글자 쌍을 병합하여 새로운 단어 사전 생성\n",
    "    merged_pair = ''.join(pair) # 가장 빈번한 글자 쌍을 합침\n",
    "    for word in v_in: # 단어 카운트 사전의 모든 키에 대해서 \n",
    "        new_word = word.replace(' '.join(pair),merged_pair) # 만약 'l o'를 갖고 있다면 'l o'를 'lo'로 replace한 새로운 단어 생성\n",
    "        v_out[new_word] = v_in[word] # output dictionary의 단어를 그 단어의 인덱스로 바꾼다\n",
    "        \n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5a46c",
   "metadata": {},
   "source": [
    "merge_dict(pair, v_in) 함수 작성\n",
    "-> 가장 빈번한 글자쌍 병합하여 새로운 단어 사전 생성\n",
    "\n",
    "함수 입력 값\n",
    "* pair\n",
    "    * 가장 빈도수가 높은 pair tuple\n",
    "    * 예: ('e', 's')\n",
    "    * **->따라서 이걸 찾아주는 과정이 필요함**\n",
    "* v_in\n",
    "    * 현재의 단어 사전(Dictionary)\n",
    "    * 예: {'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}\n",
    "\n",
    "\n",
    "함수 출력 값\n",
    "* v_out\n",
    "    * 업데이트된 단어 사전(Dictionary)\n",
    "    * 예: {'l o w': 5, 'l o w e r': 2, 'n e w es t': 6, 'w i d es t': 3}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe1dbe",
   "metadata": {},
   "source": [
    "병합 반복 과정:\n",
    "* for i in range(10) 반복문을 통해 10번의 병합 과정을 수행합니다.\n",
    "* 각 반복마다 현재 단어 사전의 상태를 출력하고, 현재의 반복 횟수를 나타냅니다.\n",
    "\n",
    "가장 빈번한 글자 쌍 찾기:\n",
    "* get_stats(count_dict) 함수를 호출하여 현재 단어 사전에서 모든 가능한 글자 쌍과 그 빈도수를 계산합니다.\n",
    "* 이후, 이 글자 쌍 중 가장 높은 빈도수를 가진 쌍을 찾습니다.\n",
    "찾은 글자 쌍과 그 빈도수를 출력합니다.\n",
    "\n",
    "사전 업데이트:\n",
    "* 가장 빈번한 글자 쌍으로 count_dict 사전을 업데이트하는 merge_dict 함수를 호출합니다.\n",
    "* 업데이트된 단어 사전을 출력합니다.\n",
    "\n",
    "단어 집합 업데이트:\n",
    "* 새롭게 병합된 글자 쌍을 단어 집합(vocabulary)에 추가합니다.\n",
    "* 업데이트된 단어 집합을 출력합니다.\n",
    "\n",
    "병합 중단 조건:\n",
    "* 만약 더 이상 병합할 글자 쌍이 없다면(if not pairs), 반복을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2735873c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 단어 사전: {'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}\n",
      "\n",
      "반복 횟수 1 ...\n",
      "('e', 's') 쌍의 빈도수가 9로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'l o w': 5, 'l o w e r': 2, 'n e w es t': 6, 'w i d es t': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es']\n",
      "\n",
      "반복 횟수 2 ...\n",
      "('es', 't') 쌍의 빈도수가 9로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'l o w': 5, 'l o w e r': 2, 'n e w est': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est']\n",
      "\n",
      "반복 횟수 3 ...\n",
      "('l', 'o') 쌍의 빈도수가 7로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'lo w': 5, 'lo w e r': 2, 'n e w est': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo']\n",
      "\n",
      "반복 횟수 4 ...\n",
      "('lo', 'w') 쌍의 빈도수가 7로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'n e w est': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low']\n",
      "\n",
      "반복 횟수 5 ...\n",
      "('n', 'e') 쌍의 빈도수가 6로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'ne w est': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne']\n",
      "\n",
      "반복 횟수 6 ...\n",
      "('ne', 'w') 쌍의 빈도수가 6로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'new est': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne', 'new']\n",
      "\n",
      "반복 횟수 7 ...\n",
      "('new', 'est') 쌍의 빈도수가 6로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'newest': 6, 'w i d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest']\n",
      "\n",
      "반복 횟수 8 ...\n",
      "('w', 'i') 쌍의 빈도수가 3로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'newest': 6, 'wi d est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi']\n",
      "\n",
      "반복 횟수 9 ...\n",
      "('wi', 'd') 쌍의 빈도수가 3로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'newest': 6, 'wid est': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid']\n",
      "\n",
      "반복 횟수 10 ...\n",
      "('wid', 'est') 쌍의 빈도수가 3로 가장 높아 merge 수행\n",
      "업데이트된 사전: {'low': 5, 'low e r': 2, 'newest': 6, 'widest': 3}\n",
      "업데이트된 단어 집합: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest']\n"
     ]
    }
   ],
   "source": [
    "print(f'초기 단어 사전: {count_dict}')\n",
    "\n",
    "for i in range(10): # 병합할 횟수\n",
    "    print()\n",
    "    print(\"반복 횟수\", i + 1, '...')\n",
    "    # 현재 사전에 대해 가장 빈번한 글자 쌍을 찾기\n",
    "    # pair 사전 만들기\n",
    "    pairs = get_stats(count_dict)\n",
    "    \n",
    "    if not pairs:\n",
    "        break  # 병합할 글자 쌍이 없으면 반복을 중단\n",
    "\n",
    "    # 가장 빈번한 글자 쌍을 찾기\n",
    "    best = ''\n",
    "    max_freq = 0\n",
    "    for pair, freq in pairs.items(): # ex> {('l','o'):5, ...}\n",
    "        if freq > max_freq:\n",
    "            best = pair\n",
    "            max_freq = freq\n",
    "    print(f'{best} 쌍의 빈도수가 {max_freq}로 가장 높아 merge 수행')\n",
    "\n",
    "    # 가장 빈번한 글자 쌍으로 사전을 업데이트\n",
    "    count_dict = merge_dict(best, count_dict)\n",
    "    print(\"업데이트된 사전:\", count_dict)\n",
    "\n",
    "    # 새로운 병합을 vocabulary에 추가\n",
    "    vocabulary.append(best[0]+best[1])\n",
    "    print(\"업데이트된 단어 집합:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470e7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjlee/sj_virtual/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 18.5kB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 289/289 [00:00<00:00, 121kB/s]\n",
      "Downloading config.json: 100%|██████████| 425/425 [00:00<00:00, 182kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '나', '##는', '오늘', '아침밥', '##을', '먹', '##었', '##다', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 토큰화 예시 문장\n",
    "text = \"나는 오늘 아침밥을 먹었다.\" ### ... 이 부분을 완성하시오'\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "encoded = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(encoded)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2c458a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 717, 2259, 3822, 30949, 2069, 1059, 2359, 2062, 18, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86e3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998776912689209}]\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 라이브러리 불러오기\n",
    "from transformers import pipeline\n",
    "# 파이프라인 라이브러리 호출하기: 감성분석\n",
    "# 원래는 모델을 지정해줘야 하지만 여기서는 기본 모델인 distilbert를 사용\n",
    "sentiment_clf = pipeline('sentiment-analysis')\n",
    "# \"what a beutiful day!\"라는 문장의 감성 확인하기\n",
    "result = sentiment_clf(\"what a beautiful day\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becdc450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9998776912689209}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "084b1c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e055e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998776912689209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb2aa948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 4.83kB/s]\n",
      "Downloading config.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 1.67MB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 279k/279k [00:00<00:00, 715kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 441M/441M [00:39<00:00, 11.3MB/s] \n",
      "Some weights of the model checkpoint at monologg/koelectra-base-finetuned-sentiment were not used when initializing ElectraForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-finetuned-sentiment and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.5034942626953125}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\")\n",
    "sentiment_clf = pipeline(\"sentiment-analysis\",tokenizer = tokenizer,model = model)\n",
    "result = sentiment_clf(\"날이 참 좋아요!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "457ea0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7689927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5034942626953125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6467a8",
   "metadata": {},
   "source": [
    "# 파이프라인을 이용한 질의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfec0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''Text mining, also referred to as text data mining (abbr.: TDM),\n",
    "        similar to text analytics,\n",
    "        is the process of deriving high-quality information from text. It involves\n",
    "        \"the discovery by computer of new, previously unknown information,\n",
    "        by automatically extracting information from different written resources.\"\n",
    "        Written resources may include websites, books, emails, reviews, and articles.\n",
    "        High-quality information is typically obtained by devising patterns and trends\n",
    "        by means such as statistical pattern learning. According to Hotho et al.(2005)\n",
    "        we can distinguish between three different perspectives of text mining:\n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databas\n",
    "        es) process.'''\n",
    "\n",
    "question = \"What is text mining?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303eaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading config.json: 100%|██████████| 473/473 [00:00<00:00, 207kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 261M/261M [00:22<00:00, 11.6MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 11.1kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 543kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 2.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.42474299669265747, 'start': 110, 'end': 168, 'answer': 'the process of deriving high-quality information from text'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 질의응답 파이프라인 호출하기\n",
    "question_answerer = pipeline(\"question-answering\") ### ... 이 부분을 완성하시오\n",
    "\n",
    "# 질문과 문단 전달하기\n",
    "answer = question_answerer(question=question, context= context) ### ... 이 부분을 완성하시오\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b004da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the process of deriving high-quality information from text'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98ebdf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.08618853241205215,\n",
       " 'start': 688,\n",
       " 'end': 734,\n",
       " 'answer': 'information extraction, data mining, and a KDD'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2 = \"what are the perspectives of text mining?\"\n",
    "answer2 = question_answerer(question = question2, context = context)\n",
    "\n",
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1501e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'information extraction, data mining, and a KDD'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6f5681c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'수원 화성은 조선시대 화성유수부 시가지를 둘러싼 성곽이다.1789년(정조 13) 수원을 팔달산 동쪽 아래로 옮기고,1794년(정조 18) 축성을 시작해 1796년에 완성했다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"수원 화성은 조선시대 화성유수부 시가지를 둘러싼 성곽이다.\n",
    "1789년(정조 13) 수원을 팔달산 동쪽 아래로 옮기고,\n",
    "1794년(정조 18) 축성을 시작해 1796년에 완성했다.\"\"\"\n",
    "context = context.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "question = \"수원 화성은 언제 완성되었는가?\"\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56309d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<00:00, 15.0kB/s]\n",
      "Downloading config.json: 100%|██████████| 472/472 [00:00<00:00, 217kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 255k/255k [00:00<00:00, 429kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 50.9kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 54.8M/54.8M [00:04<00:00, 11.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9962994456291199, 'start': 85, 'end': 91, 'answer': '1796년에'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 오토 토크나이저, 오토 모델(질의응답) 불러오기\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering ### ... 이 부분을 완성하시오\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\") ### ... 이 부분을 완성하시오\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\") ### ... 이 부분을 완성하시오\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model) ### ... 이 부분을 완성하시오\n",
    "answer = question_answerer({\"question\": question, \"context\": context}) ### ... 이 부분을 완성하시오\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57d1967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang(\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "284b7b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['최현욱',\n",
       " '더 팩트 뮤직 어워즈',\n",
       " '박지후',\n",
       " 'NewJeans',\n",
       " 'LG PC 그램',\n",
       " '케플러 (음악 그룹)',\n",
       " '김민지',\n",
       " '위버스 (플랫폼)',\n",
       " '오스트레일리아',\n",
       " '두에인 스위어진스키']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제목을 검색(search)하시오\n",
    "wiki_title = wikipedia.search('뉴진스') ### ... 이 부분을 완성하시오\n",
    "wiki_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7c1d356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NewJeans(뉴진스)는 2022년 7월 22일에 데뷔한 대한민국의 5인조 걸 그룹으로, 소속사는 하이브 산하의 레이블인 어도어이다. SM 엔터테인먼트 비주얼 디렉터 출신으로 하이브에 영입된 민희진이 프로듀서로 나서서 발굴한 걸 그룹이다. 2022년 8월 18일에 《엠카운트다운》에서 데뷔 3주만에 첫 1위에 올랐다. 2023년 8월에 잼버리 폐회식 K-POP 공연에 출연했다.\\n\\n\\n== 그룹명 ==\\n그룹으로 사용되고 있는 \\'뉴진스(NewJeans)\\'는 두 가지 의미를 함축하고 있다는 뜻에서 비롯된 말이다. 진스(jeans), 즉 청바지는 시대를 초월한 패션 아이템이라는 생각과 시대를 초월한 이미지를 스스로 개척하려는 그룹의 의도를 암시한다. 이 이름은 또한 새로운 세대(new generation)의 팝 음악을 선도하는 그룹을 가리키는 \"new genes\"라는 문구의 말장난이기도 하다.\\n\\n\\n== 역사 ==\\n\\n\\n=== 2011~2022: 데뷔 전 활동 및 결성 ===\\nNewJeans로 데뷔하기 전에 여러 그룹 멤버가 TV, 음악, 댄스에 참여했다. 호주 태생의 다니엘 마쉬(Danielle Marsh)는 한국에 살 때 2011년까지 방영된 버라이어티 쇼인 tvN 레인보우 유치원의 정규 출연자였다. 그녀는 또한 TV 프로그램인 제시의 플레이 키친과 내 마음의 크레파스에도 출연했다. 혜인은 2017년 11월 동요그룹 우쏘걸(U.SSO Girl)의 멤버 유정(U.Jeong)으로 데뷔했다가 1년 만에 팀에서 탈퇴했다. 2020년 12월, 그녀는 PocketTV가 결성한 음악 그룹이자 유튜브 그룹인 Play With Me Club에 가입했고, 2021년 5월 3일에 그룹을 졸업했다. 베트남계 호주인 하니(Hanni Pham)는 Aemina Dance Crew의 멤버로 멜버른에서 공연을 시작했다. 하니와 민지는 방탄소년단의 2021년 뮤직비디오 \\'Permission to Dance\\'에 게스트로 출연했다.빅히트 엔터테인먼트와 쏘스 뮤직이 합작한 신인 걸그룹 준비는 같은 해 SM 엔터테인먼트의 비주얼 디렉터로서 CBO로 합류해 아트 디렉션으로 널리 인정받고 있는 민희진의 지휘 아래 2019년부터 시작됐다. 2019년 9월부터 10월까지 글로벌 오디션이 진행됐고, 2020년 초부터 그룹 캐스팅이 시작됐다. 2021년 말, 민이 레이블의 CEO로 임명된 후, 프로젝트가 하이브가 새로 설립한 독립 레이블 ADOR로 이전했다고 발표됐다. 2021년 12월부터 2022년 1월까지 2차 글로벌 오디션을 진행해 2022년 3월 라인업이 확정됐다. 여러 언론 매체에서 \\'민희진 걸그룹\\'으로 불리던 이 그룹은 당초 2021년 론칭 예정이었다. 그러나 나중에 코로나19 팬데믹으로 인해 연기됐다.\\n\\n\\n=== 2022년~현재: 소개, 데뷔, 그리고 Get Up ===\\nADOR는 2022년 7월 1일에 \\'22\\', \\'7\\', \\'22\\'라는 숫자의 애니메이션 영상 3편을 게재하며 신인 걸그룹 론칭을 예고해, 7월 22일 콘텐츠 공개에 대한 추측을 불러일으켰다. 7월 22일 데뷔 싱글 \\'Attention\\' 뮤직비디오를 사전 프로모션이나 라인업 정보 없이 깜짝 공개했다. 빌보드는 이 움직임을 \"위험하지만 궁극적으로 활력이 넘친다\"고 묘사했으며, 그 성공을 \"무엇보다 음악에 중점을 둔 것\"으로 평가했다. 이 비디오에는 2개의 추가 싱글을 포함하여 4개의 트랙이 포함된 다가오는 시조 데뷔 연장 플레이가 발표되었다. 7월 23일, 그룹의 두 번째 싱글 \"Hype Boy\"가 멤버들의 이름을 공개하는 클립과 함께 공개되었으며, 멤버들의 관점에 맞는 노래에 대한 다른 4개의 뮤직 비디오도 함께 공개되었다. \\'Hype Boy\\'는 K팝 여성 가수가 빌보드 글로벌 200에서 35주 동안 차트에 진입한 최장 기간 노래가 됐다. 이틀 뒤 수록곡 \\'Hurt\\' 뮤직비디오가 공개됐다. EP의 선주문량은 3일 만에 444,000장을 넘어섰다.8월 1일, NewJeans는 세 번째 싱글 \"Cookie\"와 함께 데뷔 EP를 디지털 방식으로 발표했다. 이 노래는 가사에 성적인 풍자가 포함되어 있다고 생각하는 일부 평론가들로부터 비판을 받았다. ADOR는 비난을 부인하고 가사가 \"CD를 굽는 것과 쿠키를 굽는 것의 짝을 이루는 아이디어, 한국어에서 동일한 개념 동사를 공유하는 것\"을 의미한다고 밝혔다. 이 EP의 오프라인 버전은 8월 8일 발매되어 100만 장 이상 판매되어 대한민국 K팝 여성 가수의 베스트셀러 데뷔 앨범이자 2011년 서클 차트 설립 이후 이를 달성한 유일한 데뷔 앨범이 되었다. NewJeans는 나중에 한국대중음악상에서 최우수 K-pop 앨범을 수상했다. 이 그룹은 8월 4일 Mnet \\'엠카운트다운\\'을 통해 세 곡의 싱글을 모두 선보이며 방송 데뷔했다. NewJeans는 골든디스크어워즈, 서울가요대상, 한국대중가요대상, 멜론뮤직어워드, 아시아아티스트어워즈 등 각종 한국음악 시상식에서 신인상을 다수 수상했다.\\nNewJeans는 2022년 12월 19일 첫 번째 싱글 앨범 OMG의 첫 번째 싱글로 \"Ditto\"를 발매했다. \"Ditto\"는 한국 서클 디지털 차트에서 13주 동안 1위를 차지하며 가장 오랫동안 1위를 차지한 곡이 되었다. 이는 NewJeans의 첫 번째 빌보드 핫 100 차트 진입으로 최고 82위, 영국 싱글 차트 95위에 올랐다. OMG는 2023년 1월 2일에 발매되었다. 리뷰어들은 이 앨범의 복고풍 스타일을 칭찬했다. 발매 첫 주에 70만 장을 팔아 서클 앨범 차트 1위로 데뷔했다. New Jeans도 100만 장 판매를 달성하기 직전에 100만 장 이상 판매된 첫 번째 앨범이 되었다. OMG에는 같은 이름의 두 번째 싱글이 포함되어 TikTok에서 입소문이 났다. \\'OMG\\'는 빌보드 핫 100에서 74위를 기록하며 차트 최고 순위에 올랐다. 2023년 4월, NewJeans는 Coca-Cola Zero Sugar를 홍보하기 위해 코카콜라와 협력하여 \"Zero\"를 출시했다. 2023년 5월에는 Jon Batiste, J.I.D, Camilo와 함께 코카콜라와 공동으로 두 번째 싱글 \"Be Who You Are (Real Magic)\"를 발매했다.NewJeans는 2023년 7월 1~2일 SK 올림픽 핸드볼 체육관에서 첫 번째 매진 팬미팅 Bunnies Camp를 개최했다. 이 그룹은 2023년 7월 21일 두 번째 EP인 Get Up을 발매했다. EP는 2위로 데뷔했다. 서클 앨범 차트에 진입하며 발매 첫 주 165만 장의 판매고를 기록, 3연속 100만 장 이상 판매를 달성한 앨범이 됐다. \"Super Shy\", \"ETA\", \"Cool with You\"라는 세 개의 싱글이 지원되었다. \"Super Shy\"는 Circle Digital Chart에서 1위를 차지하여 NewJeans의 한국 세 번째 1위 싱글을 획득했으며 여러 국제 차트에서 최고의 성적을 거둔 트랙이 되었다. 트랙의 상업적 성공으로 NewJeans는 Billboard Emerging Artists 차트에서 처음으로 1위를 달성했다. 모든 곡에는 IT 기업 애플, 미디어 프랜차이즈인 The Powerpuff Girls, 한국 여배우 호연, 홍콩 배우 Tony Leung 등 다양한 브랜드 및 유명인과의 협업이 포함된 뮤직 비디오가 제공되었다. 2023년 8월에는 롤라팔루자(Lollapalooza)에서 미국 첫 라이브 공연을 펼쳤고, 이 페스티벌에서 공연하는 K팝 걸그룹 최초가 됐다. 이 그룹은 2023년 9월 1일 김종서의 \\'Beautiful Restriction\\'을 리메이크한 \\'A Time Called You\\' OST를 발매했다.2023년 9월 26일, 리그 오브 레전드 개발사 라이엇 게임즈는 NewJeans가 10월 10일부터 11월 19일까지 대한민국에서 개최되는 리그 오브 레전드 월드 챔피언십 2023의 주제가인 \"Gods\"를 공연할 것이라고 발표했다.\\n\\n\\n== 구성원 ==\\n\\n\\n== 음반 ==\\n\\n\\n=== EP ===\\n2022년 8월 1일 《New Jeans》\\n2023년 7월 21일 《Get Up》\\n\\n\\n=== 싱글 ===\\n2022년 12월 19일 《Ditto》 (선공개곡)\\n2023년 1월 2일 《OMG》 (1집)\\n2023년 4월 3일 《Zero》\\n2023년 7월 7일 《Super Shy》 (선공개곡)\\n\\n\\n== 음반 외 활동 ==\\nSBS 《뉴진스 코드 in 부산》\\ntvN 《유 퀴즈 온 더 블럭》 게스트\\n\\n\\n== 수상 목록 ==\\n\\n\\n=== 가요 프로그램 1위 ===\\n\\n\\n=== 시상식 ===\\n2022년 더 팩트 뮤직 어워즈 넥스트 리더상\\n2022년 멜론 뮤직 어워즈 TOP 10\\n2022년 멜론 뮤직 어워즈 올해의 신인상\\n2022년 아시아 아티스트 어워즈 올해의 퍼포먼스상\\n2022년 아시아 아티스트 어워즈 가수 부문 여자 신인상\\n2023년 골든 디스크 시상식 디지털음원 부문 본상\\n2023년 골든 디스크 시상식 여자 신인상\\n2023년 제32회 하이원 서울가요대상 여자 신인상\\n2023년 30주년 한터뮤직 어워즈 2022 올해의 루키상 (여성 아티스트)\\n2023년 써클차트 뮤직 어워즈 디지털 음원 부문 올해의 신인상\\n2023년 제6회 더 팩트 뮤직 어워즈 리스너스 초이스상\\n2023년 제6회 더 팩트 뮤직 어워즈 올해의 아티스트상\\n2023년 엠넷 아시안 뮤직 어워즈 베스트 댄스 퍼포먼스 여자 그룹상\\n2023년 엠넷 아시안 뮤직 어워즈 베스트 여자 그룹상\\n2023년 엠넷 아시안 뮤직 어워즈 삼성갤럭시 송 오브 더 이어 (올해의 노래상)\\n2023년 엠넷 아시안 뮤직 어워즈 삼성갤럭시 아티스트 오브 더 이어 (올해의 가수상)\\n2023년 제15회 멜론뮤직어워즈 TOP 10\\n\\n\\n== 각주 ==\\n\\n\\n== 외부 링크 ==\\n\\nNewJeans  - 공식 웹사이트\\nNewJeans_ADOR - 트위터\\nNewJeans_jp - 트위터\\nNewJeans - 인스타그램 \\nNewJeans - 틱톡 \\nNewJeans의 채널 - 유튜브'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위키피디아 페이지(page)를 가져오시오.\n",
    "wiki_text = wikipedia.page('NewJeans').content ### ... 이 부분을 완성하시오\n",
    "wiki_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f13e361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = wiki_text\n",
    "context = context.strip().replace(\"\\n\",\"\")\n",
    "question = \"뉴진스는 몇 인조 그룹이야?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dae61fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9953400492668152, 'start': 39, 'end': 42, 'answer': '5인조'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "answer = question_answerer({\"question\": question, \"context\": context})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc40f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sj_virtual",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
