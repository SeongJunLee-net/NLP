{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe86ffe",
   "metadata": {},
   "source": [
    "https://github.com/wikibook/textmining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe43ba",
   "metadata": {},
   "source": [
    "# 1장 Summary\n",
    "* `데이터 마이닝`이란 대규모 데이터베이스에서 유의미한 패턴을 찾아 이를 활용하는 방법론이다.\n",
    "    * `텍스트 마이닝`이란 텍스트의 유의미한 패턴을 찾아 이를 활용하는 방법론이다.\n",
    "         * 이러한 텍스트 마이닝의 주요 적용분야는 다음과 같은 것들이 있다 \n",
    "           - `문서분류`: 주어진 문서에 대해 미리 정의된 클래스로 분류하는 작업으로 예를 들어 메일의 내용을 분석해 스팸 메일의 여부를 확인하는 것이 있다. 이 책에서는 주로 문서 분류를 중심으로 내용을 전개한다.\n",
    "           - `문서생성`: 사람이 쓴 것과 유사한 문장을 만들어내는 작업\n",
    "           - `문서요약`: 주어진 문서에서 중요하고 흥미 있는 내용을 추출해 요약문을 생성하는 작업\n",
    "           - `질의응답`: 주어진 문장(context)을 읽고 주어진 문제(Question)에 대해 올바른 답(answer)를 생성하는 작업\n",
    "           - `기계번역`\n",
    "           - `토픽모델링`: 여러 문서에서 공통으로 등장하는 토픽(주제)를 추출하는 방법\n",
    "\n",
    "<br>\n",
    "\n",
    "* 일반적인 머신러닝 모델들은 정형화된 입력을 받기 때문에 텍스트 또한 정형화 시켜 주어야 한다.\n",
    "    * -> 벡터로써 정형화 시키고 이와 같이 주어진 텍스트를 일정한 길이의 벡터로 변환하는 것을 '임베딩'이라고 한다.\n",
    "    \n",
    "<br>\n",
    "\n",
    "* `텍스트, 문장, 문서`사이의 관계는 텍스트가 제일 큰 범위이고 문서가 두번째 큰 범위 문장이 제일 작은 범위의 용어이다.\n",
    "    * &rightarrow; 문서는 하나의 일관된 목적 혹은 주제를 갖고 쓰여진 글\n",
    "    * &rightarrow; 문장은 생각이나 감정을 말로 표현할 때 완결된 내용을 나타내는 최소 단위이다.\n",
    "    * &rightarrow; 텍스트는 분석의 대상이 되는 문자열로써 문서또는 문장을 의미할 수 있고 길이에 대한 제약이 없기 때문에 때로는 한권의 책이 될 수도 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "* `Corpus`는 자연어처리나 텍스트 마이닝을 위해 문서들을 모아놓은 집합이라고 할 수 있다.\n",
    "    * Ex> 영화 리뷰를 분석하고 싶다면 대상이 되는 리뷰들을 모아놓은 것을 말뭉치라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbbfea",
   "metadata": {},
   "source": [
    "# 2장 텍스트 전처리\n",
    "자연어 처리는 다음의 3단계로 나눌 수 있다.\n",
    "1. 자연어로 쓰여진 글을 전처리하는 준비단계\n",
    "2. 전처리된 결과를 컴퓨터가 다루고 이해할 수 있는 형태로 변환하는 단계\n",
    "3. 그리고 변환된 형태를 이용해 다양한 분석을 수행하는 단계\n",
    "\n",
    "**-> 2장에서는 자연어로 쓰여진 글을 전처리하는 단계를 배울 것이다**\n",
    "\n",
    "## 왜 전처리가 필요할까?\n",
    "우리는 어떤 문장이 있을때 각각의 단어를 이해하고 그 단어들의 순서에 따라 의미를 이해한다.<br>\n",
    "**따라서 컴퓨터에게 문장 혹은 문서를 이해시키고 싶다면 문장을 단어 단위로 나눈 후에 이 단어들의 리스트 형태로 변환해 줌으로써 순서를 부여해주어야 한다.**\n",
    "\n",
    "## 전처리의 단계\n",
    "전처리는 크게 4가지 단계로 나뉜다.\n",
    "1. `정제(cleaning)`: 분석에 불필요한 노이즈를 제거하는 작업으로써 불용어(stopword)제거가 이에 해당한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. `토큰화(Tokenization)`: 주어진 텍스트를 원하는 단위로 나누는 작업을 말한다.<br> 원하는 단위가 문장인 경우 문장 토큰화 단어인 경우 단어 토큰화라고 한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. `정규화`: 같은 의미를 가진 단어임에도 불구하고 다른 형태로 쓰여진 단어들을 통일시켜서 표준 단어로 만드는 작업을 말한다.<br>이 작업에는 어간 추출(stemming) 표제어 추출(Lemmatization)등이 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. `품사태깅`: 품사는 단어를 문법적인 기능에 따라 분류한 것을 말한다. 같은 단어도 문맥에 따라 의미와 품사가 바뀔수 있으므로 정확한 품사를 알려면 **문맥을 파악해야한다.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e08d9",
   "metadata": {},
   "source": [
    "## 1. 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d16bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sjlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sjlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sjlee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sjlee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sjlee/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590aea7d",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054c8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cafbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"it's good to see you.\", \"Let's start our NLP class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. it's good to see you. Let's start our NLP class!\"\n",
    "print(sent_tokenize(para)) #주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 등을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc25092",
   "metadata": {},
   "source": [
    "&rightarrow; 출력결과를 보면 각 문장을 문자열로 갖는 리스트임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df824b",
   "metadata": {},
   "source": [
    "sent_tokenize()는 영어 학습 데이터에 대해 사전학습된 모델을 사용해 토큰화한다. 다른 언어에 대해 문장 토큰화를 하려면 사전학습된 모델을 지정해 불러올 수 있다.\n",
    "* 하지만 아직 한글에 대해 사전학습된 모델은 존재하지 않는다.\n",
    "     * 하지만 문장 토큰화는 각 문장의 끝에 있는 마침표 등을 기준으로 분리하도록 학습돼있으므로 영어로 학습된 사전학습 모델도 한국어에 대해 어느정도 잘 작동할 것이라고 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b4ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "260c3df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee000afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "print(sent_tokenize(para_kor)) #한국어에 대해서도 sentence tokenizer는 잘 동작함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f03fe",
   "metadata": {},
   "source": [
    "### 단어 토큰화 \n",
    "일반적으로 토큰화라고 하면 단어 토큰화(word tokenize)를 의미한다. word_tokenize()함수를 이용해서 tokenize를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c0596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f403f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para)) #주어진 text를 word 단위로 tokenize함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bb400",
   "metadata": {},
   "source": [
    "WordPunctTokenizer를 사용했을때 word_tokenize와의 차이점을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb4ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d353a946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'it', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf7e1e",
   "metadata": {},
   "source": [
    "**word_tokenize와 다르게 WordPunctTokenizer는 it's를 it과 ' 그리고 s 이 세 단어로 분리하는 것을 확인할 수 있다.**<br>\n",
    "**&rightarrow; 이는 두 토크나이저가 서로 다른 알고리즘에 기반하기 때문이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474f92f",
   "metadata": {},
   "source": [
    "* 한글에 대해서도 수행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46597d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14591462",
   "metadata": {},
   "source": [
    "약간 어색하게 느껴지는 것을 볼 수 있는데, 그 이유는 한글에서의 단어는 의미를 가지는 최소단위 형태소로 분리가 가능하다. 따라서 한글에서의 `토큰화`는 형태소로 분리하는 것을 의미한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "영어에서는 단어와 단어 사이가 공백으로 분리할 수 있어 어렵지 않게 토큰화를 할 수 있지만 한글에서의 형태소는 보통 붙어있는 경우가 많기 때문에 이러한 분리 방식으로는 부족하게 느껴진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581a48e",
   "metadata": {},
   "source": [
    "## 2. 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cf6c5",
   "metadata": {},
   "source": [
    "### 노이즈와 불용어제거\n",
    "**&rightarrow; NLTK에서는 stopwords라는 라이브러리를 이용해 언어별 불용어 사전을 제공한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9dcbfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer # 정규표현식을 이용하여 Tokenize하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd7d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
    "english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
    "# 정규표현식의 의미는 '문자'+'숫자'+'_'가 1개이상인 것들을 의미한다.\n",
    "tokens = tokenizer.tokenize(text1.lower()) #word_tokenize로 토큰화\n",
    "result = [word for word in tokens if word not in english_stops] #stopwords를 제외한 단어들만으로 list를 생성\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "236e1127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ain', 'any', 'where', 'can', 'why', \"you'd\", 'will', 'into', 'isn', 'them', \"needn't\", 'shan', \"haven't\", 'yours', 'hadn', 'o', 'on', 'have', 'more', 'weren', 'who', 'in', 'you', 'been', 'yourself', 'as', 'by', 'down', 'we', 'wouldn', 'just', 'with', 'theirs', 'own', 'from', 'd', \"wouldn't\", \"you've\", 'an', 'or', \"wasn't\", 'am', 'is', 'until', 'hasn', 'such', \"should've\", 'under', 'y', 'shouldn', 'was', \"mustn't\", 'it', 'about', 'having', 're', 'he', 'she', 'those', 'during', 'did', \"shan't\", 'should', 'they', 's', 'her', 'your', \"you'll\", 'this', 'that', \"she's\", 'only', \"don't\", 'nor', \"mightn't\", 'haven', 'him', 'very', 'aren', 'each', 'too', 'hers', 'mustn', \"it's\", 'be', 'once', 't', 'between', 'there', 'below', 'of', \"isn't\", 'm', 'again', 'these', 'but', 'all', 'were', 'what', \"you're\", 'which', \"didn't\", 'our', 'and', 'whom', 'i', 'a', \"that'll\", 'for', 'll', \"shouldn't\", 'won', 'because', 'himself', 'how', 'couldn', 'against', 'further', 'up', 'not', 'through', 'other', 'so', 'now', 'my', 'me', 'myself', \"aren't\", 'than', 'didn', 'doing', 'wasn', 'their', 'while', 'same', 'had', 'if', 'here', 'ourselves', 'do', 'off', 'herself', 'when', 'few', 'over', \"couldn't\", 'out', 'don', \"hadn't\", 'yourselves', 'ma', \"hasn't\", 've', 'after', 'itself', 'has', 'doesn', 'ours', 'needn', 'themselves', 'most', 'being', 'its', \"doesn't\", \"won't\", 'are', 'the', 'his', 'both', 'some', 'above', 'no', 'does', 'at', 'mightn', \"weren't\", 'before', 'then', 'to'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english'))) # 영어 불용어가 어떤 것이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a435d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#자신만의 stopwords를 만들고 이용\n",
    "#한글처리에서도 유용하게 사용할 수 있음\n",
    "my_stopword = ['i', 'go', 'to'] # 나만의 stopword를 리스트로 정의\n",
    "result = [word for word in tokens if word not in my_stopword]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7ed29",
   "metadata": {},
   "source": [
    "## 3. 정규화\n",
    "정규화는 같은 의미를 가진 동일한 단어이면서 다른 형태로 쓰여진 단어들을 통일해 표준 단어로 만드는 작업을 말한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "예를 들어 go가 he, she 같은 3인칭 단수와 쓰일 때는 goes가 되는데, 필요에 따라 go와 goes를 같은 단어로 취급해야 할 때가 있다. 이와 같이 go의 **다양한 변형을 원형의 형태로 통일하는 작업을 정규화**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b697e",
   "metadata": {},
   "source": [
    "### Stemming(어간 추출)\n",
    "어간 추출은 \"어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해 내는 작업\"을 말한다.\n",
    "* 어형: 단어의 형태\n",
    "* 용언: 문장 안에서 서술하는 구실을 하는 동사와 형용사\n",
    "* 어간: 어형변화에서 변화하지 않는 부분[용언에서 바뀌지 않는 부분]\n",
    "* 어미: 어형변화에서 변화하는 부분[용언에서 바뀌는 부분]\n",
    "\n",
    "* ex> `간다, 갔다` & `작다, 작고 작으니`에서 `가`는 바뀌지 않는 부분이고 또한 `작`이 바뀌지 않는 부분이다. 즉, 바뀌지 않는 `가`,`작`과 같은 부분을 어간이라고 하고 그 외에 나머지 부분을 어미라고 한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "* 영어와 우리말은 원리와 구조가 다르므로 어간 추출도 달라질 수 밖에 없다. 영어의 경우에는 명사가 복수형으로 기술된 것을 단수형으로 바꾸는 것도 어간 추출에 포함된다. \n",
    "\n",
    "<br>\n",
    "\n",
    "* 영어에 대한 어간추출 알고리즘으로는 포터 스테머(Porter Stemmer)와 렝카스터 스테머(Lancaster Stemmer)등이 잘 알려졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7289519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727fe71",
   "metadata": {},
   "source": [
    "* 우리는 cookery의 어간으로 cookeri를 제시한 것을 볼 수 있는데 이는 뒤의 y를 i로 대체하는 규칙에 따라 생성된 것으로 사전에 있는 단어가 아니다. 이와 같이 Stemmer 알고리즘은 **단어가 변형되는 규칙을 이용해 원형을 찾으므로 결과가 항상 사전에 있는 올바른 단어가 되지 않는다.**\n",
    "\n",
    "* 지금 당장은 이렇게 허술하게 분석해도 되나 싶지만, 나중가면 왜 이렇게 해도 되는지 알게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81c44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our Natural Langage Processing class!\"\n",
    "tokens = word_tokenize(para)\n",
    "print(tokens)\n",
    "result = [stemmer.stem(token) for token in tokens]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46dea4e",
   "metadata": {},
   "source": [
    "* LancasterStemmer는 PorterStemmer랑 다른 알고리즘을 사용하기 때문에 결과도 조금 다르다. 어떠한 것을 사용할지는 결과를 보고 결정하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d423d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c240d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0b476",
   "metadata": {},
   "source": [
    "###  표제어 추출(Lemmatization)\n",
    "* **표제어(Lemma)는 우리말로 '단어의 기본형'으로 번역된다.** 즉, 표제어 추출은 주어진 단어를 기본형으로 변환하는 것을 의미한다.\n",
    "* `어간`과 `단어의 기본형`의 차이? \n",
    "    * `어간`과 `단어의 기본형`의 차이는 **사전에 나오는 단어인지 아닌지**의 차이로 구분할 수 있다.<br>\n",
    "    cf> '작다'의 어간은 '작'인데, 사전에 '작'이 나오기는 하지만 '작다'의 의미가 아닌 다른 의미로 나온다. 즉 스테머는 이런 단어를 언어학적인 관점에서 형태를 분리하는 것이기 때문에, 사전에 없어도 당황할 필요가 없다.<br><br>\n",
    "    하지만 Lemmatizer는 **사전에 있는** 단어의 기본형으로 바꿔준다는 점에서 차이점이 있다.\n",
    "    물론 사전에 있는 단어의 기본형으로 바꿀 때는 동사인지 명사인지 지정해줄 수 있다. `pos` 파라미터를 사용\n",
    "    <br>\n",
    "    \n",
    "    * 우리는 이제 유명한 어휘 데이터베이스인 WordNet을 이용한 `WordNetLemmatizer`를 사용하여 Lemma를 추출해 볼 것이다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6660d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40cd71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('cooking',pos = 'v'),lemmatizer.lemmatize('cookery'),lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217cba2",
   "metadata": {},
   "source": [
    "# [[[실습 2-1-1]]] 어간 추출(Stemming)과 표제어 추출(Lemmatization)의 차이점은?\n",
    "\n",
    "\n",
    "*   어간 추출과 표제어 추출의 차이점을 보일 수 있는 예시를 들고, 둘의 차이점이 무엇인지 분석해 보시오.\n",
    "\n",
    "[참고]\n",
    "```\n",
    "print('stemming result:', stemmer.stem('believes'))\n",
    "print('lemmatizing result:', lemmatizer.lemmatize('believes'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfb715",
   "metadata": {},
   "source": [
    "## stemming과 lemmatization 차이점\n",
    "stemming은 원래 정해져있는 어형변화의 규칙에 따라서 decomposing하는 것을 확인할 수 있고<br>\n",
    "lemmatize는 사전에 있는 단어를 기준으로 표제어(lemma)로 변환하는 것을 확인 할 수 있다.<br>\n",
    "따라서 stemming의 결과는 사전에 있음을 보장할 수 없지만, lemmatize의 결과는 사전에 있음을 보장할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c40fb5",
   "metadata": {},
   "source": [
    "## 4. 품사 태깅\n",
    "앞서 배운 토큰화와 정규화 과정을 거쳐서 나온 각 결과를 보통 형태소 라고 한다.\n",
    "* 형태소는 **의미를 가지는 가장 작은 말의 단위**로써 더 나누게되면 본래의 의미를 잃어버린다.\n",
    "    * 예를 들어, 책가방을 책과 가방으로 나누면 각각의 뜻을 살펴보고 책가방이라는 본래의 의미를 알 수 있다. 하지만, 가방을 '가'와 '방'으로 나누게 되면 설령 '가'와 '방'이 의미를 갖고 있다고 할지라도, '가방'이 가지는 본래의 의미는 사라진다.\n",
    "    \n",
    "* NLTK는 간소화된 공용 품사 태그 집합외에 `펜 트리뱅크 태그 집합(Penn Treebank Tagset)`을 제공하는데, 이 태그 집합은 공용 품사 태그 집합에 비해 훨씬 세분화된 품사 분류를 사용한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **영어로 된 텍스트에 대해 품사 태깅**을 하고자 한다면 NLTK를 쓰는 것이  가장 편하다.\n",
    "* `nltk.pos_tag()`는 단어토큰화된 결과에 대해 품사를 태깅해 **(단어, 품사)로 구성된 튜플**의 리스트로 품사 태깅 결과를 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e8795bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('it', 'PRP'), (\"'\", \"''\"), ('s', 'RB'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'\", \"''\"), ('s', 'JJ'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "token = tokenizer.tokenize(para)\n",
    "result = nltk.pos_tag(token)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1af3cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "# 품사의 약어를 잘 모르겠다면 nltk.help.upenn_tagset('궁금한 약어')를 사용해 의미를 파악 가능하다.\n",
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaca01a",
   "metadata": {},
   "source": [
    "* 리스트 컴프리헨션을 사용해 원하는 품사만 추출 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45795adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN','VB','JJ']\n",
    "my_words = [word for word,tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba963724",
   "metadata": {},
   "source": [
    "### 단어에 품사 정보를 추가해 구분\n",
    "단어 뒤에 품사 태그를 붙여 사용할 수 있다.<br>\n",
    "BOW를 활용한 문서 분류에서 품사 정보를 추가한 경우와 그렇지 않은 경우의 성능 차이를 비교해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b365f",
   "metadata": {},
   "source": [
    "- ''.join(리스트)\n",
    "''.join(리스트)를 이용하면 매개변수로 들어온 ['a', 'b', 'c'] 이런 식의 리스트를 'abc'의 문자열로 합쳐서 반환해주는 함수인 것이다.\n",
    "\n",
    "- '구분자'.join(리스트)\n",
    "'구분자'.join(리스트)를 이용하면 리스트의 값과 값 사이에 '구분자'에 들어온 구분자를 넣어서 하나의 문자열로 합쳐준다.\n",
    "'_'.join(['a', 'b', 'c']) 라 하면 \"a_b_c\" 와 같은 형태로 문자열을 만들어서 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65546627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello/NNP',\n",
       " 'everyone/NN',\n",
       " './.',\n",
       " 'it/PRP',\n",
       " \"'/''\",\n",
       " 's/RB',\n",
       " 'good/JJ',\n",
       " 'to/TO',\n",
       " 'see/VB',\n",
       " 'you/PRP',\n",
       " './.',\n",
       " 'Let/VB',\n",
       " \"'/''\",\n",
       " 's/JJ',\n",
       " 'start/VB',\n",
       " 'our/PRP$',\n",
       " 'text/NN',\n",
       " 'mining/NN',\n",
       " 'class/NN',\n",
       " '!/.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokenizer.tokenize(para))]\n",
    "words_with_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172cb322",
   "metadata": {},
   "source": [
    "## 한글 형태소 분석과 품사태깅\n",
    "* 영어의 토큰화를 배울 때와 마찬가지로 우리말을 토큰화 하기 위해서는 뜻을 가진 가장 작은 단위인 형태소까지 분리하는 것이 맞다. **이렇게 형태소 까지 분리를 하고나서 분석에 큰 의미가 없을 수 있는 것들은 삭제하는 것이 일반적이다.**<br><br>\n",
    "    * 예를 들어서 `하늘이 참 높고 푸르다`라는 말은 `하늘/이/참/높/고/푸르/다` 로 형태소를 나눌 수 있는데, `하늘`은 의미가 있으므로 남겨두는 것이 맞지만, `이`와 같은 조사나 `고`와 같은 어미는 문장 의미 파악에 별 도움을 주지 않으므로 삭제하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d67668",
   "metadata": {},
   "source": [
    "* NLTK를 이용하면 한국어 문서에 대해서 품사태깅은 물론이고, 토큰화조차 잘 하지 못한다.\n",
    "    * 따라서 KoNLPy를 사용해야한다.\n",
    "    * KoNLPy가 제공하는 형태소 분석기는 여러가지가 있는데 이중 Twitter와 Komoran을 사용해볼 것이다.\n",
    "        * &rightarrow; Twitter는 `Okt()`클래스로 부를수 있다.\n",
    "        * 이 형태소 분석기를 이용해 형태소 단위로 분리 및 품사를 반환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d78aec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jpype1\n",
      "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
      "\u001b[K     |████████████████████████████████| 465 kB 359 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/sjlee/sj_virtual/lib/python3.8/site-packages (from jpype1) (22.0)\n",
      "Installing collected packages: jpype1\n",
      "Successfully installed jpype1-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jpype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b60c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /home/sjlee/sj_virtual/lib/python3.8/site-packages (from konlpy) (1.23.5)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /home/sjlee/sj_virtual/lib/python3.8/site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/sjlee/sj_virtual/lib/python3.8/site-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: packaging in /home/sjlee/sj_virtual/lib/python3.8/site-packages (from JPype1>=0.7.0->konlpy) (22.0)\n",
      "Installing collected packages: konlpy\n",
      "Successfully installed konlpy-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be4b01",
   "metadata": {},
   "source": [
    "#### Okt()의 메소드\n",
    "* `morphs(phrase)`: 주어진 텍스트를 **형태소 단위**로 분리한다.\n",
    "    * 반환 결과는 형태소의 리스트\n",
    "* `nouns(phrase)`: 주어진 텍스트를 **형태소 단위로 분리하여 명사만**을 반환한다.\n",
    "    * 반환 결과는 명사의 리스트\n",
    "* `pos(phrase)`: 주어진 텍스트를 **형태소 단위로 분리하고 형태소에 품사를 부착**해 반환한다. 반환되는 형태는 (단어, 품사)로 구성된 튜플의 리스트이다.\n",
    "    * NLTK와 다른점은 품사 태깅함수가 토큰화까지 함께 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c874e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7a517b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3862bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['절망',\n",
       " '의',\n",
       " '반대',\n",
       " '가',\n",
       " '희망',\n",
       " '은',\n",
       " '아니다',\n",
       " '.',\n",
       " '\\n',\n",
       " '어',\n",
       " '두운',\n",
       " '밤하늘',\n",
       " '에',\n",
       " '별',\n",
       " '이',\n",
       " '빛나듯',\n",
       " '\\n',\n",
       " '희망',\n",
       " '은',\n",
       " '절망',\n",
       " '속',\n",
       " '에',\n",
       " '싹트는',\n",
       " '거지',\n",
       " '\\n',\n",
       " '만약',\n",
       " '에',\n",
       " '우리',\n",
       " '가',\n",
       " '희망',\n",
       " '함',\n",
       " '이',\n",
       " '적다면',\n",
       " '\\n',\n",
       " '그',\n",
       " '누가',\n",
       " '세상',\n",
       " '을',\n",
       " '비출어줄까',\n",
       " '.',\n",
       " '\\n',\n",
       " '정희성',\n",
       " ',',\n",
       " '희망',\n",
       " '공부']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.morphs(sentence) # 형태소 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2cf20f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['절망',\n",
       " '반대',\n",
       " '희망',\n",
       " '어',\n",
       " '두운',\n",
       " '밤하늘',\n",
       " '별',\n",
       " '희망',\n",
       " '절망',\n",
       " '속',\n",
       " '거지',\n",
       " '만약',\n",
       " '우리',\n",
       " '희망',\n",
       " '함',\n",
       " '그',\n",
       " '누가',\n",
       " '세상',\n",
       " '정희성',\n",
       " '희망',\n",
       " '공부']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nouns(sentence) # 형태소 단위로 토큰화 이후 명사 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e590f154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('절망', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('반대', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('희망', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('아니다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('어', 'Noun'),\n",
       " ('두운', 'Noun'),\n",
       " ('밤하늘', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('별', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('빛나듯', 'Verb'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('희망', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('절망', 'Noun'),\n",
       " ('속', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('싹트는', 'Verb'),\n",
       " ('거지', 'Noun'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('만약', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('우리', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('희망', 'Noun'),\n",
       " ('함', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('적다면', 'Verb'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('그', 'Noun'),\n",
       " ('누가', 'Noun'),\n",
       " ('세상', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('비출어줄까', 'Verb'),\n",
       " ('.', 'Punctuation'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('정희성', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('희망', 'Noun'),\n",
       " ('공부', 'Noun')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.pos(sentence) # 형태소 단위로 토큰화하고 품사를 붙여서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e88672a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "k = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08474b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['절망',\n",
       " '의',\n",
       " '반대',\n",
       " '가',\n",
       " '희망',\n",
       " '은',\n",
       " '아니',\n",
       " '다',\n",
       " '.',\n",
       " '어둡',\n",
       " 'ㄴ',\n",
       " '밤하늘',\n",
       " '에',\n",
       " '별',\n",
       " '이',\n",
       " '빛나',\n",
       " '듯',\n",
       " '희망',\n",
       " '은',\n",
       " '절망',\n",
       " '속',\n",
       " '에',\n",
       " '싹트',\n",
       " '는',\n",
       " '거지',\n",
       " '만약',\n",
       " '에',\n",
       " '우리',\n",
       " '가',\n",
       " '희망',\n",
       " '하',\n",
       " 'ㅁ',\n",
       " '이',\n",
       " '적',\n",
       " '다면',\n",
       " '그',\n",
       " '누구',\n",
       " '가',\n",
       " '세상',\n",
       " '을',\n",
       " '비추',\n",
       " 'ㄹ',\n",
       " '어',\n",
       " '주',\n",
       " 'ㄹ까',\n",
       " '.',\n",
       " '정희성',\n",
       " ',',\n",
       " '희망',\n",
       " '공부']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1041cf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['절망',\n",
       " '반대',\n",
       " '희망',\n",
       " '밤하늘',\n",
       " '별',\n",
       " '희망',\n",
       " '절망',\n",
       " '속',\n",
       " '거지',\n",
       " '만약',\n",
       " '희망',\n",
       " '세상',\n",
       " '정희성',\n",
       " '희망',\n",
       " '공부']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.nouns(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3230d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('절망', 'NNG'),\n",
       " ('의', 'JKG'),\n",
       " ('반대', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('희망', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('아니', 'VCN'),\n",
       " ('다', 'EF'),\n",
       " ('.', 'SF'),\n",
       " ('어둡', 'VA'),\n",
       " ('ㄴ', 'ETM'),\n",
       " ('밤하늘', 'NNP'),\n",
       " ('에', 'JKB'),\n",
       " ('별', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('빛나', 'VV'),\n",
       " ('듯', 'EC'),\n",
       " ('희망', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('절망', 'NNG'),\n",
       " ('속', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('싹트', 'VV'),\n",
       " ('는', 'ETM'),\n",
       " ('거지', 'NNG'),\n",
       " ('만약', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('우리', 'NP'),\n",
       " ('가', 'JKS'),\n",
       " ('희망', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㅁ', 'ETN'),\n",
       " ('이', 'JKS'),\n",
       " ('적', 'VA'),\n",
       " ('다면', 'EC'),\n",
       " ('그', 'MM'),\n",
       " ('누구', 'NP'),\n",
       " ('가', 'JKS'),\n",
       " ('세상', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('비추', 'VV'),\n",
       " ('ㄹ', 'ETM'),\n",
       " ('어', 'EC'),\n",
       " ('주', 'VX'),\n",
       " ('ㄹ까', 'EF'),\n",
       " ('.', 'SF'),\n",
       " ('정희성', 'NNP'),\n",
       " (',', 'SP'),\n",
       " ('희망', 'NNG'),\n",
       " ('공부', 'NNG')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.pos(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc341f27",
   "metadata": {},
   "source": [
    "# [[[실습2-1-2]]] 형태소 분석기 비교: Okt vs Komoran\n",
    "* Okt 자리에 Komoran을 입력해 보세요.\n",
    "\n",
    "\n",
    "```\n",
    "from konlpy.tag import Komoran\n",
    "t = Komoran()\n",
    "```\n",
    "그리고, Okt와 Komoran의 결과를 비교해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc82df9",
   "metadata": {},
   "source": [
    "# Okt vs Komoran\n",
    "Okt는 ㄴ과 같은 글자까지 분해하지 않고 Komoran은 ㄴ과 같이 가장 기본적인 단어까지 분해하는 것을 볼 수 있다.<br>\n",
    "의존형태소까지 분해하냐 실질형태소까지 분해하냐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0244a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sj_virtual",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
